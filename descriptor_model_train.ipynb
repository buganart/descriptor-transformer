{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "descriptor_model_train.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/buganart/descriptor-transformer/blob/main/descriptor_model_train.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jbp-CL5ijb4e",
        "cellView": "form"
      },
      "source": [
        "#@markdown Before starting please save the notebook in your drive by clicking on `File -> Save a copy in drive`"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XQ-pH7tyK9xW",
        "cellView": "form"
      },
      "source": [
        "#@markdown Check GPU, should be a Tesla V100\n",
        "!nvidia-smi -L\n",
        "import os\n",
        "print(f\"We have {os.cpu_count()} CPU cores.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BJyxzcLOhgWY",
        "cellView": "form"
      },
      "source": [
        "#@markdown Mount google drive\n",
        "from google.colab import drive\n",
        "from google.colab import output\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "from pathlib import Path\n",
        "if not Path(\"/content/drive/My Drive/IRCMS_GAN_collaborative_database\").exists():\n",
        "    raise RuntimeError(\n",
        "        \"Shortcut to our shared drive folder doesn't exits.\\n\\n\"\n",
        "        \"\\t1. Go to the google drive web UI\\n\"\n",
        "        \"\\t2. Right click shared folder IRCMS_GAN_collaborative_database and click \\\"Add shortcut to Drive\\\"\"\n",
        "    )\n",
        "\n",
        "def clear_on_success(msg=\"Ok!\"):\n",
        "    if _exit_code == 0:\n",
        "        output.clear()\n",
        "        print(msg)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9-L3BlfGTfbJ",
        "cellView": "form"
      },
      "source": [
        "#@markdown Install wandb and log in\n",
        "%pip install wandb\n",
        "output.clear()\n",
        "import wandb\n",
        "from pathlib import Path\n",
        "wandb_drive_netrc_path = Path(\"drive/My Drive/colab/.netrc\")\n",
        "wandb_local_netrc_path = Path(\"/root/.netrc\")\n",
        "if wandb_drive_netrc_path.exists():\n",
        "    import shutil\n",
        "\n",
        "    print(\"Wandb .netrc file found, will use that to log in.\")\n",
        "    shutil.copy(wandb_drive_netrc_path, wandb_local_netrc_path)\n",
        "else:\n",
        "    print(\n",
        "        f\"Wandb config not found at {wandb_drive_netrc_path}.\\n\"\n",
        "        f\"Using manual login.\\n\\n\"\n",
        "        f\"To use auto login in the future, finish the manual login first and then run:\\n\\n\"\n",
        "        f\"\\t!mkdir -p '{wandb_drive_netrc_path.parent}'\\n\"\n",
        "        f\"\\t!cp {wandb_local_netrc_path} '{wandb_drive_netrc_path}'\\n\\n\"\n",
        "        f\"Then that file will be used to login next time.\\n\"\n",
        "    )\n",
        "\n",
        "!wandb login\n",
        "output.clear()\n",
        "print(\"ok!\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BVjGm8m_q9R6"
      },
      "source": [
        "#@title Configuration\n",
        "\n",
        "#@markdown Directories can be found via file explorer on the left by navigating into `drive` to the desired folders. \n",
        "#@markdown Then right-click and `Copy path`.\n",
        "audio_db_dir = \"/content/drive/My Drive/AUDIO DATABASE/MUSIC TRANSFORMER/Transformer Corpus\" #@param {type:\"string\"}\n",
        "# audio_db_dir = \"/content/drive/My Drive/AUDIO DATABASE/TESTING\" #@param {type:\"string\"}\n",
        "experiment_dir = \"/content/drive/My Drive/IRCMS_GAN_collaborative_database/Experiments/colab-violingan/descriptor-model\" #@param {type:\"string\"}\n",
        "\n",
        "#@markdown ### Resumption of previous runs\n",
        "#@markdown Optional resumption arguments below, leaving both empty will start a new run from scratch. \n",
        "#@markdown - The ID can be found on wandb. \n",
        "#@markdown - It's 8 characters long and may contain a-z letters and digits (for example `1t212ycn`).\n",
        "\n",
        "#@markdown Resume a previous run \n",
        "resume_run_id = \"\" #@param {type:\"string\"}\n",
        "\n",
        "#@markdown train argument\n",
        "selected_model = \"LSTMEncoderDecoderModel\" #@param [\"LSTM\", \"LSTMEncoderDecoderModel\", \"TransformerEncoderOnlyModel\", \"TransformerModel\"]\n",
        "remove_outliers=True#@param {type: \"boolean\"}\n",
        "descriptor_size = 5 #@param {type: \"integer\"}\n",
        "window_size = 200 #@param {type: \"integer\"}\n",
        "#@markdown - positional_embedding optional for \"LSTMEncoderDecoderModel\", \"TransformerModel\", necessary for \"TransformerEncoderOnlyModel\"\n",
        "add_positional_embedding=True#@param {type: \"boolean\"}\n",
        "dim_pos_encoding=20     #@param {type: \"integer\"}\n",
        "#@markdown - num_layer for each model (including num_encoder_layer/num_decoder_layer)\n",
        "num_layers = 3 #@param {type: \"integer\"}\n",
        "\n",
        "learning_rate = 1e-4 #@param {type: \"number\"}\n",
        "batch_size = 64 #@param {type: \"integer\"}\n",
        "epochs = 3000 #@param {type: \"integer\"}\n",
        "\n",
        "# log_interval = 10 #@param {type: \"integer\"}\n",
        "save_interval = 10 #@param {type: \"integer\"}\n",
        "# n_test_samples = 8 #@param {type: \"integer\"}\n",
        "\n",
        "\n",
        "\n",
        "notes = \"\" #@param {type: \"string\"}\n",
        "#@markdown model specific argument\n",
        "#@markdown - LSTMEncoderDecoder, TransformerModel (forecast_size)\n",
        "forecast_size=200 #@param {type: \"integer\"}\n",
        "#@markdown - LSTM\n",
        "hidden_size=100 #@param {type: \"integer\"}\n",
        "#@markdown - TransformerEncoder, TransformerModel\n",
        "nhead=5     #@param {type: \"integer\"}\n",
        "dropout=0.1     #@param {type: \"number\"}\n",
        "dim_feedforward=128     #@param {type: \"integer\"}\n",
        "\n",
        "import re\n",
        "from pathlib import Path\n",
        "from argparse import Namespace\n",
        "\n",
        "audio_db_dir = Path(audio_db_dir)\n",
        "experiment_dir = Path(experiment_dir)\n",
        "\n",
        "\n",
        "for path in [experiment_dir]:\n",
        "    path.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "if not audio_db_dir.exists():\n",
        "    raise RuntimeError(f\"audio_db_dir {audio_db_dir} does not exists.\")\n",
        "\n",
        "def check_wandb_id(run_id):\n",
        "    if run_id and not re.match(r\"^[\\da-z]{8}$\", run_id):\n",
        "        raise RuntimeError(\n",
        "            \"Run ID needs to be 8 characters long and contain only letters a-z and digits.\\n\"\n",
        "            f\"Got \\\"{run_id}\\\"\"\n",
        "        )\n",
        "\n",
        "check_wandb_id(resume_run_id)\n",
        "\n",
        "colab_config = {\n",
        "    \"audio_db_dir\": audio_db_dir,\n",
        "    \"experiment_dir\": experiment_dir,\n",
        "    \"resume_run_id\": resume_run_id,\n",
        "    \"remove_outliers\": remove_outliers,\n",
        "    \"descriptor_size\": descriptor_size,\n",
        "    \"window_size\": window_size,\n",
        "    \"forecast_size\": forecast_size,\n",
        "    \"learning_rate\": learning_rate,\n",
        "    \"batch_size\": batch_size,\n",
        "    \"epochs\": epochs,\n",
        "    \"save_interval\": save_interval,\n",
        "    \"selected_model\": selected_model,\n",
        "    \"notes\": notes,\n",
        "    \"hidden_size\": hidden_size,\n",
        "    \"num_layers\": num_layers,\n",
        "    \"dim_pos_encoding\": dim_pos_encoding,\n",
        "    \"nhead\": nhead,\n",
        "    \"dropout\": dropout,\n",
        "    \"dim_feedforward\": dim_feedforward,\n",
        "}\n",
        "\n",
        "for k, v in colab_config.items():\n",
        "    print(f\"=> {k:20}: {v}\")\n",
        "\n",
        "config = Namespace(**colab_config)\n",
        "config.seed = 1234\n",
        "\n",
        "if config.selected_model not in [\"LSTMEncoderDecoderModel\", \"TransformerModel\"]:\n",
        "    config.forecast_size = 0\n",
        "config.window_size = config.window_size + config.forecast_size"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5hCJPdJzKqCW",
        "cellView": "form"
      },
      "source": [
        "#@markdown Install dependency\r\n",
        "%pip install --upgrade git+https://github.com/buganart/descriptor-transformer.git#egg=desc\r\n",
        "import torch\r\n",
        "from desc.train_function import save_model_args, get_resume_run_config, init_wandb_run, setup_datamodule, setup_model, train\r\n",
        "clear_on_success()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k5w7a7NBws3i"
      },
      "source": [
        "#Train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mAlQ8vbQIvnE"
      },
      "source": [
        "run = init_wandb_run(config, run_dir=experiment_dir)#, mode=\"offline\")\r\n",
        "datamodule = setup_datamodule(config, run)\r\n",
        "model, extra_trainer_args = setup_model(config, run)\r\n",
        "if torch.cuda.is_available():\r\n",
        "    extra_trainer_args[\"gpus\"] = -1\r\n",
        "train(config, run, model, datamodule, extra_trainer_args)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}