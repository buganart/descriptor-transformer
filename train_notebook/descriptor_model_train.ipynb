{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "descriptor_model_train.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/buganart/descriptor-transformer/blob/main/train_notebook/descriptor_model_train.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jbp-CL5ijb4e",
        "cellView": "form"
      },
      "source": [
        "#@markdown Before starting please save the notebook in your drive by clicking on `File -> Save a copy in drive`"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XQ-pH7tyK9xW",
        "cellView": "form"
      },
      "source": [
        "#@markdown Check GPU, should be a Tesla V100\n",
        "!nvidia-smi -L\n",
        "import os\n",
        "print(f\"We have {os.cpu_count()} CPU cores.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BJyxzcLOhgWY",
        "cellView": "form"
      },
      "source": [
        "#@markdown Mount google drive\n",
        "from google.colab import drive\n",
        "from google.colab import output\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "from pathlib import Path\n",
        "if not Path(\"/content/drive/My Drive/IRCMS_GAN_collaborative_database\").exists():\n",
        "    raise RuntimeError(\n",
        "        \"Shortcut to our shared drive folder doesn't exits.\\n\\n\"\n",
        "        \"\\t1. Go to the google drive web UI\\n\"\n",
        "        \"\\t2. Right click shared folder IRCMS_GAN_collaborative_database and click \\\"Add shortcut to Drive\\\"\"\n",
        "    )\n",
        "\n",
        "def clear_on_success(msg=\"Ok!\"):\n",
        "    if _exit_code == 0:\n",
        "        output.clear()\n",
        "        print(msg)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9-L3BlfGTfbJ",
        "cellView": "form"
      },
      "source": [
        "#@markdown Install wandb and log in\n",
        "%pip install wandb\n",
        "output.clear()\n",
        "import wandb\n",
        "from pathlib import Path\n",
        "wandb_drive_netrc_path = Path(\"drive/My Drive/colab/.netrc\")\n",
        "wandb_local_netrc_path = Path(\"/root/.netrc\")\n",
        "if wandb_drive_netrc_path.exists():\n",
        "    import shutil\n",
        "\n",
        "    print(\"Wandb .netrc file found, will use that to log in.\")\n",
        "    shutil.copy(wandb_drive_netrc_path, wandb_local_netrc_path)\n",
        "else:\n",
        "    print(\n",
        "        f\"Wandb config not found at {wandb_drive_netrc_path}.\\n\"\n",
        "        f\"Using manual login.\\n\\n\"\n",
        "        f\"To use auto login in the future, finish the manual login first and then run:\\n\\n\"\n",
        "        f\"\\t!mkdir -p '{wandb_drive_netrc_path.parent}'\\n\"\n",
        "        f\"\\t!cp {wandb_local_netrc_path} '{wandb_drive_netrc_path}'\\n\\n\"\n",
        "        f\"Then that file will be used to login next time.\\n\"\n",
        "    )\n",
        "\n",
        "!wandb login\n",
        "output.clear()\n",
        "print(\"ok!\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QBPQFBntYAkC"
      },
      "source": [
        "# Description\n",
        "\n",
        "This notebook is used for training descriptor model and log results to the wandb project \"demiurge/descriptor_model\". This notebook is based on the code from [buganart/descriptor-transformer](https://github.com/buganart/descriptor-transformer).\n",
        "\n",
        "To start training the descriptor model, user will need to \n",
        "\n",
        "1. specify **audio_db_dir** to locate a music folder or a descriptor folder in the mounted Google Drive. If the folder is a music folder, the code will process all the music files (.wav) to descriptpor files (.json). **hop_length** and **sr(sampling rate)** are parameters to process music files into descriptors. The extracted descriptors describe a music segment with librosa features such as *spectral_centroid*, *spectral_flatness*, *spectral_rolloff*, and *rms*.  \n",
        "\n",
        "2. specify **selected_model** in \"LSTM\", \"LSTMEncoderDecoderModel\", \"TransformerEncoderOnlyModel\", or \"TransformerModel\". The models will take processed descriptors as input with sequence length **window_size**, and try to predict subsequent descriptors based on the input. For \"LSTMEncoderDecoder\" and \"TransformerModel\", the model encoder read the input with sequence length **window_size**, and the model decoder generate output with sequence length **forecast_size**. However, for \"LSTM\" and \"TransformerEncoderOnlyModel\", the model will just try to predict the next one descriptor based on the input.\n",
        "\n",
        "3. run the code, and record the wandb run id. The wandb run id will be used for resuming run and for generate descriptors in the [prediction notebook](https://github.com/buganart/descriptor-transformer/blob/main/descriptor_model_predict.ipynb).\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "## Models\n",
        "\n",
        "* [\"LSTM\"](https://pytorch.org/tutorials/beginner/nlp/sequence_models_tutorial.html#example-an-lstm-for-part-of-speech-tagging)\n",
        "    * simple time series next step prediction model built based on nn.LSTM module.\n",
        "    * this model will only predict the next one step descriptor given all descriptors of all sequence length of the input\n",
        "    * the model will process the input with nn.LSTM module, and then the resulting hidden vector will be processed by nn.Linear module to generate output descriptors.\n",
        "\n",
        "* [\"LSTMEncoderDecoderModel\"](https://arxiv.org/pdf/1406.1078.pdf)\n",
        "    * encoder decoder model implemented using nn.LSTM module\n",
        "    * after the encoder nn.LSTM module process the input descriptors of all sequence length into a hidden vector, the decoder nn.LSTM module can decode the hidden vector to descriptors of sequence length in **forecast_size**.\n",
        "* [\"TransformerEncoderOnlyModel\"](https://pytorch.org/tutorials/beginner/transformer_tutorial.html)\n",
        "    * based on pytorch Transformer implementation using only nn.TransformerEncoder.\n",
        "    * this model will only predict the next one step descriptor after the nn.TransformerEncoder module process the input descriptors\n",
        "* [\"TransformerModel\"](https://pytorch.org/docs/stable/generated/torch.nn.Transformer.html)\n",
        "    * based on pytorch Transformer implementation\n",
        "    * setting the input descriptors as the source(*src*) vector, and the zero vector of sequence length in **forecast_size** as target(*tgt*) vector, the model try to predict the output descriptors of sequence length in **forecast_size**\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "##Training Parameters\n",
        "\n",
        "**experiment_dir**: \n",
        "\n",
        "the path where the data generated from the melgan training process is saved.\n",
        "\n",
        "**resume_run_id**: \n",
        "\n",
        "In case the run is stopped, and the user want to resume such run, please specify wandb run id in the **resume_run_id**.\n",
        "\n",
        "---\n",
        "\n",
        "**remove_outliers**: \n",
        "\n",
        "For all the descriptors extracted from the **audio_db_dir** for training, the mean and std will be calculated. If **remove_outliers** is set to True, descriptors that the values are far from the mean will be removed.\n",
        "\n",
        "**process_on_the_fly**: \n",
        "\n",
        "decide whether to process descriptors into batch before training or during training.\n",
        "\n",
        "*   If *True*, the extracted descriptors for each music sample file (.wav) are stored in the dataset list separately. When a data batch is needed in the training process, the sample index (which sample to draw data) and window index (which segment of the sample) will be determined and returned. \n",
        "*   If *False*, the extracted descriptors for each music sample file (.wav) will be batchified, so every segment of size **window_size** + **forecast_size** will be copied and saved in the dataset list. In the training process, random batch will be drawn from the dataset list. This approach dramatically increase the dataset storage size.\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "**window_size**: \n",
        "\n",
        "the sequence length of the descriptor model input \n",
        "\n",
        "**forecast_size**: \n",
        "\n",
        "the sequence length of the descriptor model output\n",
        "\n",
        "---\n",
        "\n",
        "**add_positional_embedding**: \n",
        "\n",
        "Sinusoidal Positional Encoding described in the transformer model paper [Attention Is All You Need](https://arxiv.org/pdf/1706.03762.pdf). The Positional Encoding is optional for \"LSTMEncoderDecoderModel\" and \"TransformerModel\", but necessary for \"TransformerEncoderOnlyModel\".\n",
        "\n",
        "**dim_pos_encoding**: \n",
        "\n",
        "the number of dimensions for the Sinusoidal Positional Encoding.\n",
        "\n",
        "**num_layers**: \n",
        "\n",
        "* \"LSTM\": The *num_layers* parameter for the nn.LSTM module\n",
        "\n",
        "* \"LSTMEncoderDecoderModel\": The *num_layers* parameter for the nn.LSTM encoder and nn.LSTM decoder.\n",
        "\n",
        "* \"TransformerEncoderOnlyModel\": The *num_layers* parameter for the nn.TransformerEncoder module.\n",
        "\n",
        "* \"TransformerModel\": The *num_encoder_layers* and *num_decoder_layers* parameter for the nn.Transformer module.\n",
        "\n",
        "**hidden_size**:\n",
        "\n",
        "only for \"LSTM\" and \"LSTMEncoderDecoderModel\". \n",
        "\n",
        "* The *hidden_size* of the nn.LSTM module in the \"LSTM\" and \"LSTMEncoderDecoderModel\".\n",
        "\n",
        "**nhead**, **dropout**, **dim_feedforward**:\n",
        "\n",
        "only for \"TransformerEncoderOnlyModel\" and \"TransformerModel\". \n",
        "\n",
        "* The *nhead*, *dropout*, *dim_feedforward* of the nn.Transformer module in the \"TransformerModel\".\n",
        "\n",
        "* The *nhead*, *dropout*, *dim_feedforward* of the nn.TransformerEncoder module in the \"TransformerEncoderOnlyModel\".\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "**learning_rate**: \n",
        "\n",
        "the learning rate to train the model\n",
        "\n",
        "**batch_size**:\n",
        "\n",
        "the batch size of the data batch drawn from the dataset to train the model per training step\n",
        "\n",
        "**epochs**:\n",
        "\n",
        "the maximum number of epoch to train the model\n",
        "\n",
        "**save_interval**:\n",
        "\n",
        "the interval in number of epoch to save a checkpoint file from pytorch lightning Trainer.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BVjGm8m_q9R6",
        "cellView": "form"
      },
      "source": [
        "#@title Configuration\n",
        "#@markdown Directories can be found via file explorer on the left by navigating into `drive` to the desired folders. \n",
        "#@markdown Then right-click and *`Copy path`*.\n",
        "\n",
        "#@markdown ### #dataset directory / train save directory\n",
        "#@markdown - the path to save experiment / model data\n",
        "experiment_dir = \"/content/drive/My Drive/IRCMS_GAN_collaborative_database/Experiments/colab-violingan/descriptor-model\" #@param {type:\"string\"}\n",
        "\n",
        "#@markdown - the path of the dataset\n",
        "#@markdown - the files in the folder can be music (.wav) or extracted descriptors (.json)\n",
        "# audio_db_dir = \"/content/drive/My Drive/AUDIO DATABASE/MUSIC TRANSFORMER/barber corpus\" #@param {type:\"string\"}\n",
        "audio_db_dir = \"/content/drive/My Drive/AUDIO DATABASE/TESTING\" #@param {type:\"string\"}\n",
        "#@markdown - wav parameters to process music (.wav) to extracted descriptors (.json)\n",
        "hop_length = 1024 #@param {type:\"integer\"}\n",
        "sr = 44100 #@param {type:\"integer\"}\n",
        "#@markdown - currently descriptors are [\"cent\", \"flat\", \"rolloff\", \"rms\", \"f0\"] based on librosa \n",
        "descriptor_size = 5 #@param {type: \"integer\"}\n",
        "\n",
        "\n",
        "\n",
        "#@markdown ### #Resumption of previous runs\n",
        "#@markdown Optional resumption arguments below, leaving it empty will start a new run from scratch. \n",
        "\n",
        "#@markdown Note that for resuming run, the config parameters will NOT be changed even the train arugments are different.\n",
        "#@markdown - The ID can be found on wandb. \n",
        "#@markdown - It's 8 characters long and may contain a-z letters and digits (for example `1t212ycn`).\n",
        "\n",
        "#@markdown Resume a previous run \n",
        "resume_run_id = \"\" #@param {type:\"string\"}\n",
        "\n",
        "#@markdown ### #train argument\n",
        "selected_model = \"LSTMEncoderDecoderModel\" #@param [\"LSTM\", \"LSTMEncoderDecoderModel\", \"TransformerEncoderOnlyModel\", \"TransformerModel\"]\n",
        "#@markdown - remove descriptors that the values are far from the mean\n",
        "remove_outliers=True#@param {type: \"boolean\"}\n",
        "#@markdown - if True, fixed number of samples will be drawn per epoch from the dataset regardless of the dataset size.\n",
        "#@markdown - if False, all sliced data will be packed and shuffled per epoch\n",
        "process_on_the_fly=True#@param {type: \"boolean\"}\n",
        "\n",
        "#@markdown - the input sequence length to train the model\n",
        "window_size = 2000 #@param {type: \"integer\"}\n",
        "#@markdown - positional_embedding optional for \"LSTMEncoderDecoderModel\", \"TransformerModel\", necessary for \"TransformerEncoderOnlyModel\"\n",
        "add_positional_embedding=True#@param {type: \"boolean\"}\n",
        "dim_pos_encoding=20     #@param {type: \"integer\"}\n",
        "#@markdown - num_layer for each model (including num_encoder_layer/num_decoder_layer)\n",
        "num_layers = 3 #@param {type: \"integer\"}\n",
        "\n",
        "learning_rate = 1e-4 #@param {type: \"number\"}\n",
        "batch_size = 64 #@param {type: \"integer\"}\n",
        "epochs = 3000 #@param {type: \"integer\"}\n",
        "\n",
        "# log_interval = 10 #@param {type: \"integer\"}\n",
        "#@markdown - how many epochs to save a model checkpoint  \n",
        "save_interval = 10 #@param {type: \"integer\"}\n",
        "# n_test_samples = 8 #@param {type: \"integer\"}\n",
        "\n",
        "\n",
        "\n",
        "notes = \"\" #@param {type: \"string\"}\n",
        "#@markdown model specific argument\n",
        "#@markdown - LSTMEncoderDecoder, TransformerModel (forecast_size)\n",
        "#@markdown - the output sequence length of the model based on the input of size \"window_size\" \n",
        "forecast_size=2000 #@param {type: \"integer\"}\n",
        "#@markdown - LSTM\n",
        "#@markdown - the hidden dim for LSTM\n",
        "hidden_size=100 #@param {type: \"integer\"}\n",
        "#@markdown - TransformerEncoder, TransformerModel\n",
        "nhead=5     #@param {type: \"integer\"}\n",
        "dropout=0.1     #@param {type: \"number\"}\n",
        "dim_feedforward=128     #@param {type: \"integer\"}\n",
        "\n",
        "import re\n",
        "from pathlib import Path\n",
        "from argparse import Namespace\n",
        "\n",
        "audio_db_dir = Path(audio_db_dir)\n",
        "experiment_dir = Path(experiment_dir)\n",
        "\n",
        "\n",
        "for path in [experiment_dir]:\n",
        "    path.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "if not audio_db_dir.exists():\n",
        "    raise RuntimeError(f\"audio_db_dir {audio_db_dir} does not exists.\")\n",
        "\n",
        "def check_wandb_id(run_id):\n",
        "    if run_id and not re.match(r\"^[\\da-z]{8}$\", run_id):\n",
        "        raise RuntimeError(\n",
        "            \"Run ID needs to be 8 characters long and contain only letters a-z and digits.\\n\"\n",
        "            f\"Got \\\"{run_id}\\\"\"\n",
        "        )\n",
        "\n",
        "check_wandb_id(resume_run_id)\n",
        "\n",
        "colab_config = {\n",
        "    \"audio_db_dir\": audio_db_dir,\n",
        "    \"hop_length\": hop_length,\n",
        "    \"sr\": sr,\n",
        "    \"experiment_dir\": experiment_dir,\n",
        "    \"resume_run_id\": resume_run_id,\n",
        "    \"remove_outliers\": remove_outliers,\n",
        "    \"descriptor_size\": descriptor_size,\n",
        "    \"window_size\": window_size,\n",
        "    \"forecast_size\": forecast_size,\n",
        "    \"learning_rate\": learning_rate,\n",
        "    \"batch_size\": batch_size,\n",
        "    \"epochs\": epochs,\n",
        "    \"save_interval\": save_interval,\n",
        "    \"selected_model\": selected_model,\n",
        "    \"notes\": notes,\n",
        "    \"hidden_size\": hidden_size,\n",
        "    \"num_layers\": num_layers,\n",
        "    \"dim_pos_encoding\": dim_pos_encoding,\n",
        "    \"nhead\": nhead,\n",
        "    \"dropout\": dropout,\n",
        "    \"dim_feedforward\": dim_feedforward,\n",
        "}\n",
        "\n",
        "for k, v in colab_config.items():\n",
        "    print(f\"=> {k:20}: {v}\")\n",
        "\n",
        "config = Namespace(**colab_config)\n",
        "config.seed = 1234\n",
        "\n",
        "if config.selected_model not in [\"LSTMEncoderDecoderModel\", \"TransformerModel\"]:\n",
        "    config.forecast_size = 0\n",
        "config.window_size = config.window_size + config.forecast_size"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5hCJPdJzKqCW",
        "cellView": "form"
      },
      "source": [
        "#@markdown Install dependency\n",
        "%pip install --upgrade git+https://github.com/buganart/descriptor-transformer.git#egg=desc\n",
        "import torch\n",
        "from desc.train_function import save_model_args, get_resume_run_config, init_wandb_run, setup_datamodule, setup_model, train\n",
        "clear_on_success()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k5w7a7NBws3i"
      },
      "source": [
        "#Train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mAlQ8vbQIvnE"
      },
      "source": [
        "run = init_wandb_run(config, run_dir=experiment_dir)#, mode=\"offline\")\n",
        "datamodule = setup_datamodule(config, run, isTrain=True, process_on_the_fly=process_on_the_fly)\n",
        "model, extra_trainer_args = setup_model(config, run)\n",
        "if torch.cuda.is_available():\n",
        "    extra_trainer_args[\"gpus\"] = -1\n",
        "train(config, run, model, datamodule, extra_trainer_args)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}