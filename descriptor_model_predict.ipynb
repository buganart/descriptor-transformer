{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "descriptor_model_predict.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "zkgjJdO--JPf",
        "liDBc0QQFtuM",
        "iypRTwjcyNZL",
        "_LM_xFvI0pfb"
      ],
      "toc_visible": true,
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/buganart/descriptor-transformer/blob/main/descriptor_model_predict.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jbp-CL5ijb4e",
        "cellView": "form"
      },
      "source": [
        "#@markdown Before starting please save the notebook in your drive by clicking on `File -> Save a copy in drive`"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XQ-pH7tyK9xW",
        "cellView": "form"
      },
      "source": [
        "#@markdown Check GPU, should be a Tesla V100\n",
        "!nvidia-smi -L\n",
        "import os\n",
        "print(f\"We have {os.cpu_count()} CPU cores.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BJyxzcLOhgWY",
        "cellView": "form"
      },
      "source": [
        "#@markdown Mount google drive\n",
        "from google.colab import drive\n",
        "from google.colab import output\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "from pathlib import Path\n",
        "if not Path(\"/content/drive/My Drive/IRCMS_GAN_collaborative_database\").exists():\n",
        "    raise RuntimeError(\n",
        "        \"Shortcut to our shared drive folder doesn't exits.\\n\\n\"\n",
        "        \"\\t1. Go to the google drive web UI\\n\"\n",
        "        \"\\t2. Right click shared folder IRCMS_GAN_collaborative_database and click \\\"Add shortcut to Drive\\\"\"\n",
        "    )\n",
        "\n",
        "def clear_on_success(msg=\"Ok!\"):\n",
        "    if _exit_code == 0:\n",
        "        output.clear()\n",
        "        print(msg)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9-L3BlfGTfbJ",
        "cellView": "form"
      },
      "source": [
        "#@markdown Install wandb and log in\n",
        "%pip install wandb\n",
        "output.clear()\n",
        "import wandb\n",
        "from pathlib import Path\n",
        "wandb_drive_netrc_path = Path(\"drive/My Drive/colab/.netrc\")\n",
        "wandb_local_netrc_path = Path(\"/root/.netrc\")\n",
        "if wandb_drive_netrc_path.exists():\n",
        "    import shutil\n",
        "\n",
        "    print(\"Wandb .netrc file found, will use that to log in.\")\n",
        "    shutil.copy(wandb_drive_netrc_path, wandb_local_netrc_path)\n",
        "else:\n",
        "    print(\n",
        "        f\"Wandb config not found at {wandb_drive_netrc_path}.\\n\"\n",
        "        f\"Using manual login.\\n\\n\"\n",
        "        f\"To use auto login in the future, finish the manual login first and then run:\\n\\n\"\n",
        "        f\"\\t!mkdir -p '{wandb_drive_netrc_path.parent}'\\n\"\n",
        "        f\"\\t!cp {wandb_local_netrc_path} '{wandb_drive_netrc_path}'\\n\\n\"\n",
        "        f\"Then that file will be used to login next time.\\n\"\n",
        "    )\n",
        "\n",
        "!wandb login\n",
        "output.clear()\n",
        "print(\"ok!\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iP1BbsXBidDo"
      },
      "source": [
        "# Description\n",
        "\n",
        "This notebook generates music (.wav) based on runs from the wandb project \"demiurge/descriptor_model\". You may access the training models through [train.ipynb](https://github.com/buganart/descriptor-transformer/blob/main/descriptor_model_train.ipynb). The user will need to specify a **test_data_path** for a sound file folder (.wav), the notebook will generate descriptors (.json) for each sound file and convert those descriptors back into the same (.wav) format. The generated sound files will be the prediction of potential subsequent sounds for the input files.\n",
        "\n",
        "To generate such predictive sound files, this notebook will first \n",
        "\n",
        "\n",
        "1.   process input music files in **test_data_path** and music descriptor database specified in **audio_dir** to descriptors if the files in the folder is not being processed into descriptors. The processed descriptors will be saved in the same path in the \"processed_descriptors\" folder. If they have already been processed, this step will be skipped. Note that **hop length** and **sampling rate(sr)** are parameters for processing music to descriptors. \n",
        "2.   load trained descriptor model from wandb project \"demiurge/descriptor_model\". Set **resume_run_id** directly, then the saved checkpoint of the run will be downloaded. The model loaded from the checkpoint will predict the subsequent descriptors based on **prediction_length**.\n",
        "3.   query the predicted descriptors to the music descriptor database specified in **audio_dir**. The predicted descriptors will be replaced by the descriptors in the music descriptor database based on distance function such as euclidean, cosine, minkowski\n",
        "4.   process the descriptor in the descriptor database and match them back to the music segment where it is extracted. Then, those music segments will be merged together into the generated music file. Note that **crossfade** is a parameter in the merging process. The generated music files will be saved in the **output_dir**.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BVjGm8m_q9R6",
        "cellView": "form"
      },
      "source": [
        "#@title CONFIGURATION\n",
        "\n",
        "#@markdown Directories can be found via file explorer on the left menu by navigating to `drive`  and then to the desired folders. \n",
        "#@markdown Then right-click and `Copy path`.\n",
        "\n",
        "#@markdown ### #descriptor model input\n",
        "\n",
        "#@markdown The descriptor will extract a .json file containing *spectral centroid/spectral flatness/fundamental frequency/spectral rolloff/RMS* data from the test_data_path .wavs below. The model will predict **prediction_length** descriptors to follow the test descriptor files.\n",
        "#@markdown - This is the **Prediction DB** containing data for the model to generate next descriptors.\n",
        "#@markdown - The model will predict next **prediction_length** descriptors given **window_size**(specified in the model) descriptors\n",
        "#@markdown - if test_data_path is a path to a music directory, descriptors will be extracted from **test_data_path** and saved in **output_dir**\n",
        "# test_data_path = \"/content/drive/My Drive/AUDIO DATABASE/MUSIC TRANSFORMER/Transformer Corpus/\" #@param {type:\"string\"}\n",
        "# test_data_path = \"/content/drive/My Drive/AUDIO DATABASE/MUSIC TRANSFORMER/sample_descriptor_files\" #@param {type:\"string\"}\n",
        "test_data_path = \"/content/drive/My Drive/AUDIO DATABASE/TESTING/\" #@param {type:\"string\"}\n",
        "\n",
        "#@markdown ### #descriptor database\n",
        "\n",
        "#@markdown - the path to the wav. file database to generate the descriptor database\n",
        "#@markdown - This is the **RAW generated audio DB** which is only for the query and playback engine.\n",
        "#@markdown - The descriptors predicted from the model need to be converted back to music. The files in this dataset will create a database with descriptor-sound mapping, which is used for converting descriptors back to music.\n",
        "audio_dir = \"/content/drive/My Drive/AUDIO DATABASE/TESTING/\" #@param {type:\"string\"}\n",
        "\n",
        "#@markdown - descriptors will be extracted from the **audio_dir** above but if your provide a input_db_filename that path will be used instead\n",
        "# input_db_filename = f\"/content/drive/My Drive/Descriptor Model/robertos_output.json\" #@param {type:\"string\"}\n",
        "# input_db_filename = \"/content/drive/My Drive/AUDIO DATABASE/TESTING/output_descriptor_database.json\" #@param {type:\"string\"}\n",
        "input_db_filename = \"\" #@param {type:\"string\"}\n",
        "\n",
        "#@markdown ### #resumption of previous runs\n",
        "#@markdown Optional resumption arguments below, leaving both empty will start a new run from scratch.\n",
        "#@markdown - The IDs can be found on Wanda. It is 8 characters long and may contain a-z letters and digits (for example **1t212ycn**)\n",
        "\n",
        "#@markdown Resume a previous run \n",
        "resume_run_id = \"lny7atep\" #@param {type:\"string\"}\n",
        "\n",
        "#@markdown ### #descriptors / sound parameter\n",
        "#@markdown - the number of predicted descriptors after the **test_data**\n",
        "prediction_length =  40#@param {type:\"integer\"}\n",
        "\n",
        "#@markdown - wav parameters (hop length, sampling rate, crossfade)\n",
        "hop_length = 1024 #@param {type:\"integer\"}\n",
        "sr = 44100 #@param {type:\"integer\"}\n",
        "crossfade = 22 #@param {type:\"integer\"}\n",
        "\n",
        "#@markdown ### #save location\n",
        "#@markdown - the path to save all generated files\n",
        "output_dir = f\"/content/drive/My Drive/Descriptor Model/OUTPUTS/{resume_run_id}\" #@param {type:\"string\"}\n",
        "# #@markdown name of generated files\n",
        "# #@markdown - the file storing generated descriptors from the model\n",
        "# generated_descriptor_filename = \"AUDIOS_output.json\" #@param {type:\"string\"}\n",
        "# #@markdown - the file storing closest match query descriptors based on generated descriptors\n",
        "# query_descriptor_filename = \"query_output.json\" #@param {type:\"string\"}\n",
        "# #@markdown - the final wav file from combining music source represented by the query descriptors\n",
        "# final_wav_filename = \"output.wav\" #@param {type:\"string\"}\n",
        "\n",
        "\n",
        "hop_length = int(hop_length)\n",
        "sr = int(sr)\n",
        "crossfade = int(crossfade)\n",
        "\n",
        "import re\n",
        "from pathlib import Path\n",
        "from argparse import Namespace\n",
        "\n",
        "def check_wandb_id(run_id):\n",
        "    if run_id and not re.match(r\"^[\\da-z]{8}$\", run_id):\n",
        "        raise RuntimeError(\n",
        "            \"Run ID needs to be 8 characters long and contain only letters a-z and digits.\\n\"\n",
        "            f\"Got \\\"{run_id}\\\"\"\n",
        "        )\n",
        "\n",
        "check_wandb_id(resume_run_id)\n",
        "\n",
        "output_dir = Path(output_dir)\n",
        "output_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "#remove existing files\n",
        "output_dir_files = output_dir.rglob(\"*.*\")\n",
        "for i in output_dir_files:\n",
        "    i.unlink()\n",
        "\n",
        "\n",
        "colab_config = {\n",
        "    \"resume_run_id\": resume_run_id,\n",
        "    \"test_data_path\": test_data_path,\n",
        "    \"prediction_length\": prediction_length,\n",
        "    \"output_dir\": output_dir,\n",
        "}\n",
        "\n",
        "for k, v in colab_config.items():\n",
        "    print(f\"=> {k:20}: {v}\")\n",
        "\n",
        "config = Namespace(**colab_config)\n",
        "config.seed = 1234"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5hCJPdJzKqCW",
        "cellView": "form"
      },
      "source": [
        "#@markdown Install dependency and functions\n",
        "%pip install --upgrade git+https://github.com/buganart/descriptor-transformer.git#egg=desc\n",
        "from desc.train_function import get_resume_run_config, init_wandb_run, setup_model, setup_datamodule\n",
        "from desc.helper_function import save_descriptor_as_json, dir2descriptor, save_json, get_dataframe_from_json\n",
        "\n",
        "%pip install --upgrade librosa\n",
        "import librosa\n",
        "\n",
        "import numpy as np\n",
        "import json\n",
        "import os, os.path\n",
        "from IPython.display import HTML, display\n",
        "import time\n",
        "import shutil\n",
        "\n",
        "import pandas as pd\n",
        "from numba import jit, cuda \n",
        "from scipy.spatial.distance import cosine, minkowski, euclidean\n",
        "import torch\n",
        "\n",
        "\n",
        "%pip install pydub\n",
        "%pip install ffmpeg\n",
        "\n",
        "from pydub import AudioSegment\n",
        "from pydub.playback import play\n",
        "\n",
        "\n",
        "def progress(value, max=100):\n",
        "    return HTML(\"\"\"\n",
        "        <progress\n",
        "            value='{value}'\n",
        "            max='{max}',\n",
        "            style='width: 100%'\n",
        "        >\n",
        "            {value}\n",
        "        </progress>\n",
        "    \"\"\".format(value=value, max=max))\n",
        "\n",
        "clear_on_success()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zkgjJdO--JPf"
      },
      "source": [
        "## WAV TO DESCRIPTOR\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P4EXGrsHCZqi"
      },
      "source": [
        "#process input descriptor database if needed\n",
        "if not input_db_filename:\n",
        "    save_path = output_dir\n",
        "    db_descriptors = dir2descriptor(audio_dir, hop=hop_length, sr=sr)\n",
        "\n",
        "    #combine descriptors from multiple files\n",
        "    data_dict = {}\n",
        "    for filename, descriptor in db_descriptors:\n",
        "        for element in descriptor:\n",
        "            if element in data_dict:\n",
        "                data_dict[element] = data_dict[element] + descriptor[element]\n",
        "            else:\n",
        "                data_dict[element] = descriptor[element]\n",
        "    \n",
        "    #replace empty input_db_filename by savefile name\n",
        "    input_db_filename = Path(save_path) / \"AUDIOS_database.json\"\n",
        "    save_json(input_db_filename, data_dict)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "liDBc0QQFtuM"
      },
      "source": [
        "## DESCRIPTOR MODEL GENERATOR\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cX-QEhDcFt3b"
      },
      "source": [
        "config = get_resume_run_config(resume_run_id)\n",
        "config.resume_run_id = resume_run_id\n",
        "config.audio_db_dir = test_data_path\n",
        "# please check window_size (if window_size is too large, 0 descriptor samples will be extracted.)\n",
        "#print(config.window_size)\n",
        "\n",
        "run = init_wandb_run(config, run_dir=\"./\", mode=\"offline\")\n",
        "model,_ = setup_model(config, run)\n",
        "model.eval()\n",
        "#construct test_data\n",
        "testdatamodule = setup_datamodule(config, run, isTrain=False)\n",
        "test_dataloader = testdatamodule.test_dataloader()\n",
        "test_data, fileindex = next(iter(test_dataloader))\n",
        "\n",
        "prediction = model.predict(test_data, prediction_length)\n",
        "\n",
        "#un normalize output\n",
        "prediction = prediction * testdatamodule.dataset_std + testdatamodule.dataset_mean\n",
        "\n",
        "generated_dir = output_dir / \"generated_descriptors\"\n",
        "generated_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "save_descriptor_as_json(generated_dir, prediction, fileindex, testdatamodule, resume_run_id)\n",
        "print(\"ok!\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iypRTwjcyNZL"
      },
      "source": [
        "## QUERY FUNCTION\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kGh7eY4UyNn7"
      },
      "source": [
        "query_dir = output_dir / \"query_descriptors\"\n",
        "query_dir.mkdir(parents=True, exist_ok=True)\n",
        "print(\"query_dir:\", query_dir)\n",
        "\n",
        "# import df1 (UnaGAN output)\n",
        "input_db_filename = Path(input_db_filename)\n",
        "df1 = get_dataframe_from_json(input_db_filename)\n",
        "\n",
        "# import df2 (Descriptor GAN output)\n",
        "generated_file_list = generated_dir.rglob(\"*.*\")\n",
        "generated_dataframe_list = []\n",
        "for filepath in generated_file_list:\n",
        "    df2 = get_dataframe_from_json(filepath)\n",
        "    generated_dataframe_list.append((filepath, df2))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5_d7aq51zk48"
      },
      "source": [
        "#####   modified (batch)\n",
        "for filepath, df2 in generated_dataframe_list:\n",
        "    #record runtime\n",
        "    current_time = time.time()\n",
        "    dict_key1 = list(df2.columns)[0]\n",
        "    input_len = len(df2[dict_key1])\n",
        "    column_list = list(df2.columns)\n",
        "    input_array = torch.tensor(df2.loc[:, column_list].to_numpy(dtype=np.float32)).cuda()\n",
        "    db = torch.tensor(df1.loc[:, column_list].to_numpy(dtype=np.float32)).cuda()\n",
        "\n",
        "\n",
        "    # not enough RAM for array of shape (input_len, db_len)\n",
        "    batch_size = 4096\n",
        "    results_all = []\n",
        "    for i in range(int(input_len/batch_size)+1):\n",
        "        x = i * batch_size\n",
        "        x_ = (i+1) * batch_size\n",
        "        if x_ > input_len:\n",
        "            x_ = input_len\n",
        "        input = input_array[x:x_]\n",
        "        dist = torch.cdist(input, db, p=2)\n",
        "        results = torch.argmin(dist, axis=1).cpu().numpy()\n",
        "        results_all.append(results)\n",
        "\n",
        "    results_all = np.concatenate(results_all).flatten()\n",
        "\n",
        "    id_array = df1[\"_id\"][results_all]\n",
        "    sample_array = df1[\"_sample\"][results_all]\n",
        "\n",
        "    data={\n",
        "        \"_id\": id_array.tolist(), \n",
        "        \"_sample\": sample_array.tolist()\n",
        "    }\n",
        "    print(\"finished - saving as JSON now\")\n",
        "\n",
        "    \n",
        "    savefile = query_dir / (str(filepath.stem) + \".json\")\n",
        "    with open(savefile, 'w') as outfile:\n",
        "        json.dump(data, outfile, indent=2)\n",
        "\n",
        "    print(\"descriptors are replaced by query descriptors in database. save file path: \", savefile)\n",
        "\n",
        "    #record runtime\n",
        "    step_time = time.time() - current_time\n",
        "    print(\"time used:\", step_time)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_LM_xFvI0pfb"
      },
      "source": [
        "## PLAYBACK ENGINE\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D7O3KWqY13sB"
      },
      "source": [
        "wav_dir = output_dir / \"wav_output\"\n",
        "wav_dir.mkdir(parents=True, exist_ok=True)\n",
        "print(\"wav_dir:\", wav_dir)\n",
        "\n",
        "query_file_list = query_dir.rglob(\"*.*\")\n",
        "query_dataframe_list = []\n",
        "for filepath in query_file_list:\n",
        "    to_play = get_dataframe_from_json(filepath)\n",
        "    query_dataframe_list.append((filepath, to_play))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tfF14v-E3QmD"
      },
      "source": [
        "for filepath, to_play in query_dataframe_list:\n",
        "    output_filename = wav_dir / (str(filepath.stem) + \".wav\")\n",
        "    \n",
        "    # output_filename = output_dir / final_wav_filename\n",
        "\n",
        "    if os.path.exists(output_filename):\n",
        "        os.remove(output_filename)\n",
        "\n",
        "    no_samples = len(to_play[\"_sample\"])\n",
        "    out = display(progress(0, no_samples), display_id = True)\n",
        "\n",
        "    concat = AudioSegment.from_wav(to_play[\"_id\"][0])\n",
        "    hop = (hop_length / sr) * 1000\n",
        "    startpos = int((float(to_play[\"_sample\"][0]) / hop_length) * hop)\n",
        "\n",
        "    concat = concat[startpos:startpos + hop]\n",
        "\n",
        "    for x in range(1, no_samples):\n",
        "        print(to_play[\"_id\"][x])\n",
        "        to_concat = AudioSegment.from_wav(to_play[\"_id\"][x])\n",
        "        startpos = int((float(to_play[\"_sample\"][x]) / hop_length) * hop)\n",
        "        if (startpos < crossfade): \n",
        "            thiscrossfade = 0\n",
        "        else: \n",
        "            to_concat = to_concat[startpos - (crossfade / 2):startpos + hop]\n",
        "            thiscrossfade = crossfade\n",
        "        out.update(progress(x + 1, no_samples))\n",
        "\n",
        "        concat = concat.append(to_concat, crossfade = thiscrossfade)\n",
        "\n",
        "    concat.export(output_filename, format = \"wav\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}