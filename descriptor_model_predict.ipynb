{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "descriptor_model_predict.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "zkgjJdO--JPf",
        "liDBc0QQFtuM",
        "iypRTwjcyNZL",
        "_LM_xFvI0pfb"
      ],
      "toc_visible": true,
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/buganart/descriptor-transformer/blob/main/descriptor_model_predict.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jbp-CL5ijb4e",
        "cellView": "form"
      },
      "source": [
        "#@markdown Before starting please save the notebook in your drive by clicking on `File -> Save a copy in drive`"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XQ-pH7tyK9xW",
        "cellView": "form"
      },
      "source": [
        "#@markdown Check GPU, should be a Tesla V100\n",
        "!nvidia-smi -L\n",
        "import os\n",
        "print(f\"We have {os.cpu_count()} CPU cores.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BJyxzcLOhgWY",
        "cellView": "form"
      },
      "source": [
        "#@markdown Mount google drive\n",
        "from google.colab import drive\n",
        "from google.colab import output\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "from pathlib import Path\n",
        "if not Path(\"/content/drive/My Drive/IRCMS_GAN_collaborative_database\").exists():\n",
        "    raise RuntimeError(\n",
        "        \"Shortcut to our shared drive folder doesn't exits.\\n\\n\"\n",
        "        \"\\t1. Go to the google drive web UI\\n\"\n",
        "        \"\\t2. Right click shared folder IRCMS_GAN_collaborative_database and click \\\"Add shortcut to Drive\\\"\"\n",
        "    )\n",
        "\n",
        "def clear_on_success(msg=\"Ok!\"):\n",
        "    if _exit_code == 0:\n",
        "        output.clear()\n",
        "        print(msg)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9-L3BlfGTfbJ",
        "cellView": "form"
      },
      "source": [
        "#@markdown Install wandb and log in\n",
        "%pip install wandb\n",
        "output.clear()\n",
        "import wandb\n",
        "from pathlib import Path\n",
        "wandb_drive_netrc_path = Path(\"drive/My Drive/colab/.netrc\")\n",
        "wandb_local_netrc_path = Path(\"/root/.netrc\")\n",
        "if wandb_drive_netrc_path.exists():\n",
        "    import shutil\n",
        "\n",
        "    print(\"Wandb .netrc file found, will use that to log in.\")\n",
        "    shutil.copy(wandb_drive_netrc_path, wandb_local_netrc_path)\n",
        "else:\n",
        "    print(\n",
        "        f\"Wandb config not found at {wandb_drive_netrc_path}.\\n\"\n",
        "        f\"Using manual login.\\n\\n\"\n",
        "        f\"To use auto login in the future, finish the manual login first and then run:\\n\\n\"\n",
        "        f\"\\t!mkdir -p '{wandb_drive_netrc_path.parent}'\\n\"\n",
        "        f\"\\t!cp {wandb_local_netrc_path} '{wandb_drive_netrc_path}'\\n\\n\"\n",
        "        f\"Then that file will be used to login next time.\\n\"\n",
        "    )\n",
        "\n",
        "!wandb login\n",
        "output.clear()\n",
        "print(\"ok!\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iP1BbsXBidDo"
      },
      "source": [
        "# Instruction\n",
        "TODO"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BVjGm8m_q9R6",
        "cellView": "form"
      },
      "source": [
        "#@title Configuration\n",
        "\n",
        "#@markdown Directories can be found via file explorer on the left by navigating into `drive` to the desired folders. \n",
        "#@markdown Then right-click and `Copy path`.\n",
        "\n",
        "#@markdown ### #descriptor model input\n",
        "\n",
        "#@markdown The test descriptor files that the descriptor model based on. The model will predict \"prediction_length\" descriptors that follows the test descriptor files.\n",
        "#@markdown - if test_data_path is a path to music directory, descriptors will be extracted from \"test_data_path\" and saved in \"output_dir\".\n",
        "# test_data_path = \"/content/drive/My Drive/AUDIO DATABASE/MUSIC TRANSFORMER/Transformer Corpus/\" #@param {type:\"string\"}\n",
        "# test_data_path = \"/content/drive/My Drive/AUDIO DATABASE/MUSIC TRANSFORMER/sample_descriptor_files\" #@param {type:\"string\"}\n",
        "test_data_path = \"/content/drive/My Drive/AUDIO DATABASE/TESTING/\" #@param {type:\"string\"}\n",
        "\n",
        "#@markdown ### #descriptor database\n",
        "#@markdown - the path to the descriptor database file (descriptors should be extracted from audio_dir below.)\n",
        "#@markdown - if input_db_filename is empty, descriptors will be extracted from \"audio_dir\" and saved in \"output_dir\".\n",
        "# input_db_filename = f\"/content/drive/My Drive/Descriptor Model/robertos_output.json\" #@param {type:\"string\"}\n",
        "# input_db_filename = \"/content/drive/My Drive/AUDIO DATABASE/TESTING/output_descriptor_database.json\" #@param {type:\"string\"}\n",
        "input_db_filename = \"\" #@param {type:\"string\"}\n",
        "#@markdown - the path music database that create the descriptor database above\n",
        "audio_dir = \"/content/drive/My Drive/AUDIO DATABASE/TESTING/\" #@param {type:\"string\"}\n",
        "\n",
        "#@markdown ### #Resumption of previous runs\n",
        "#@markdown Optional resumption arguments below, leaving both empty will start a new run from scratch. \n",
        "#@markdown - The ID can be found on wandb. \n",
        "#@markdown - It's 8 characters long and may contain a-z letters and digits (for example `1t212ycn`).\n",
        "\n",
        "#@markdown Resume a previous run \n",
        "resume_run_id = \"2pziwqbu\" #@param {type:\"string\"}\n",
        "\n",
        "#@markdown ### #descriptors / sound parameter\n",
        "#@markdown - the number of predicted descriptors after the test_data\n",
        "prediction_length =  40#@param {type:\"integer\"}\n",
        "\n",
        "#@markdown - wav parameters (hop length, sampling rate, crossfade)\n",
        "hop_length = 1024 #@param {type:\"integer\"}\n",
        "sr = 44100 #@param {type:\"integer\"}\n",
        "crossfade = 22 #@param {type:\"integer\"}\n",
        "\n",
        "#@markdown ### #save location\n",
        "#@markdown the path to save all generated files\n",
        "output_dir = f\"/content/drive/My Drive/Descriptor Model/OUTPUTS/{resume_run_id}\" #@param {type:\"string\"}\n",
        "# #@markdown name of generated files\n",
        "# #@markdown - the file storing generated descriptors from the model\n",
        "# generated_descriptor_filename = \"AUDIOS_output.json\" #@param {type:\"string\"}\n",
        "# #@markdown - the file storing closest match query descriptors based on generated descriptors\n",
        "# query_descriptor_filename = \"query_output.json\" #@param {type:\"string\"}\n",
        "# #@markdown - the final wav file from combining music source represented by the query descriptors\n",
        "# final_wav_filename = \"output.wav\" #@param {type:\"string\"}\n",
        "\n",
        "\n",
        "hop_length = int(hop_length)\n",
        "sr = int(sr)\n",
        "crossfade = int(crossfade)\n",
        "\n",
        "import re\n",
        "from pathlib import Path\n",
        "from argparse import Namespace\n",
        "\n",
        "def check_wandb_id(run_id):\n",
        "    if run_id and not re.match(r\"^[\\da-z]{8}$\", run_id):\n",
        "        raise RuntimeError(\n",
        "            \"Run ID needs to be 8 characters long and contain only letters a-z and digits.\\n\"\n",
        "            f\"Got \\\"{run_id}\\\"\"\n",
        "        )\n",
        "\n",
        "check_wandb_id(resume_run_id)\n",
        "\n",
        "output_dir = Path(output_dir)\n",
        "output_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "#remove existing files\n",
        "output_dir_files = output_dir.rglob(\"*.*\")\n",
        "for i in output_dir_files:\n",
        "    i.unlink()\n",
        "\n",
        "\n",
        "colab_config = {\n",
        "    \"resume_run_id\": resume_run_id,\n",
        "    \"test_data_path\": test_data_path,\n",
        "    \"prediction_length\": prediction_length,\n",
        "    \"output_dir\": output_dir,\n",
        "}\n",
        "\n",
        "for k, v in colab_config.items():\n",
        "    print(f\"=> {k:20}: {v}\")\n",
        "\n",
        "config = Namespace(**colab_config)\n",
        "config.seed = 1234"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5hCJPdJzKqCW",
        "cellView": "form"
      },
      "source": [
        "#@markdown Install dependency and functions\n",
        "%pip install --upgrade git+https://github.com/buganart/descriptor-transformer.git#egg=desc\n",
        "from desc.train_function import get_resume_run_config, init_wandb_run, setup_model, setup_datamodule\n",
        "from desc.helper_function import save_descriptor_as_json, dir2descriptor, save_json, get_dataframe_from_json\n",
        "\n",
        "%pip install --upgrade librosa\n",
        "import librosa\n",
        "\n",
        "import numpy as np\n",
        "import json\n",
        "import os, os.path\n",
        "from IPython.display import HTML, display\n",
        "import time\n",
        "import shutil\n",
        "\n",
        "import pandas as pd\n",
        "from numba import jit, cuda \n",
        "from scipy.spatial.distance import cosine, minkowski, euclidean\n",
        "import torch\n",
        "\n",
        "\n",
        "%pip install pydub\n",
        "%pip install ffmpeg\n",
        "\n",
        "from pydub import AudioSegment\n",
        "from pydub.playback import play\n",
        "\n",
        "\n",
        "def progress(value, max=100):\n",
        "    return HTML(\"\"\"\n",
        "        <progress\n",
        "            value='{value}'\n",
        "            max='{max}',\n",
        "            style='width: 100%'\n",
        "        >\n",
        "            {value}\n",
        "        </progress>\n",
        "    \"\"\".format(value=value, max=max))\n",
        "\n",
        "clear_on_success()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zkgjJdO--JPf"
      },
      "source": [
        "#wav to descriptor"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P4EXGrsHCZqi"
      },
      "source": [
        "#process database if needed\n",
        "if not input_db_filename:\n",
        "    save_path = output_dir\n",
        "    db_descriptors = dir2descriptor(audio_dir, hop=hop_length, sr=sr)\n",
        "\n",
        "    #combine descriptors from multiple files\n",
        "    data_dict = {}\n",
        "    for filename, descriptor in db_descriptors:\n",
        "        for element in descriptor:\n",
        "            if element in data_dict:\n",
        "                data_dict[element] = data_dict[element] + descriptor[element]\n",
        "            else:\n",
        "                data_dict[element] = descriptor[element]\n",
        "    \n",
        "    #replace empty input_db_filename by savefile name\n",
        "    input_db_filename = Path(save_path) / \"AUDIOS_database.json\"\n",
        "    save_json(input_db_filename, data_dict)\n",
        "\n",
        "#process descriptor model input if needed\n",
        "save_path = output_dir / \"descriptor_model_input\"\n",
        "save_path.mkdir(parents=True, exist_ok=True)\n",
        "# converted descriptor will also be saved in the same directory\n",
        "test_descriptors = dir2descriptor(test_data_path, hop=hop_length, sr=sr)\n",
        "for filename, descriptors in test_descriptors:\n",
        "    filename = Path(filename)\n",
        "    save_file = Path(save_path) / (str(filename.stem) + \".json\")\n",
        "    save_json(save_file, descriptors)\n",
        "\n",
        "#also, copy all .json and .txt to the target folder\n",
        "filepaths = [path for path in Path(test_data_path).rglob('*.*') if path.suffix in [\".txt\", \".json\"]]\n",
        "for filepath in filepaths:\n",
        "    print(f\"copied file \\\"{filepath}\\\" to \\\"{save_path}\\\"\")\n",
        "    shutil.copy(filepath, save_path)\n",
        "\n",
        "audio_db_dir = save_path"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "liDBc0QQFtuM"
      },
      "source": [
        "# descriptor model generate"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cX-QEhDcFt3b"
      },
      "source": [
        "config = get_resume_run_config(resume_run_id)\n",
        "config.resume_run_id = resume_run_id\n",
        "config.audio_db_dir = audio_db_dir\n",
        "# please check window_size (if window_size is too large, 0 descriptor samples will be extracted.)\n",
        "#print(config.window_size)\n",
        "\n",
        "run = init_wandb_run(config, run_dir=\"./\", mode=\"offline\")\n",
        "model,_ = setup_model(config, run)\n",
        "model.eval()\n",
        "#construct test_data\n",
        "testdatamodule = setup_datamodule(config, run, isTrain=False)\n",
        "test_dataloader = testdatamodule.test_dataloader()\n",
        "test_data, fileindex = next(iter(test_dataloader))\n",
        "\n",
        "prediction = model.predict(test_data, prediction_length)\n",
        "\n",
        "#un normalize output\n",
        "prediction = prediction * testdatamodule.dataset_std + testdatamodule.dataset_mean\n",
        "\n",
        "generated_dir = output_dir / \"generated_descriptors\"\n",
        "generated_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "save_descriptor_as_json(generated_dir, prediction, fileindex, testdatamodule, resume_run_id)\n",
        "print(\"ok!\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iypRTwjcyNZL"
      },
      "source": [
        "#query generated descriptor to stored descriptor in database"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kGh7eY4UyNn7"
      },
      "source": [
        "query_dir = output_dir / \"query_descriptors\"\n",
        "query_dir.mkdir(parents=True, exist_ok=True)\n",
        "print(\"query_dir:\", query_dir)\n",
        "\n",
        "# import df1 (UnaGAN output)\n",
        "input_db_filename = Path(input_db_filename)\n",
        "df1 = get_dataframe_from_json(input_db_filename)\n",
        "\n",
        "# import df2 (Descriptor GAN output)\n",
        "generated_file_list = generated_dir.rglob(\"*.*\")\n",
        "generated_dataframe_list = []\n",
        "for filepath in generated_file_list:\n",
        "    df2 = get_dataframe_from_json(filepath)\n",
        "    generated_dataframe_list.append((filepath, df2))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5_d7aq51zk48"
      },
      "source": [
        "#####   modified (batch)\n",
        "for filepath, df2 in generated_dataframe_list:\n",
        "    #record runtime\n",
        "    current_time = time.time()\n",
        "\n",
        "    input_len = len(df2[\"cent\"])\n",
        "    column_list = [\"f0\", \"flat\", \"rolloff\", \"rms\", \"cent\"]\n",
        "    input_array = torch.tensor(df2.loc[:, column_list].to_numpy(dtype=np.float32)).cuda()\n",
        "    db = torch.tensor(df1.loc[:, column_list].to_numpy(dtype=np.float32)).cuda()\n",
        "\n",
        "\n",
        "    # not enough RAM for array of shape (input_len, db_len)\n",
        "    batch_size = 4096\n",
        "    results_all = []\n",
        "    for i in range(int(input_len/batch_size)+1):\n",
        "        x = i * batch_size\n",
        "        x_ = (i+1) * batch_size\n",
        "        if x_ > input_len:\n",
        "            x_ = input_len\n",
        "        input = input_array[x:x_]\n",
        "        dist = torch.cdist(input, db, p=2)\n",
        "        results = torch.argmin(dist, axis=1).cpu().numpy()\n",
        "        results_all.append(results)\n",
        "\n",
        "    results_all = np.concatenate(results_all).flatten()\n",
        "\n",
        "    id_array = df1[\"_id\"][results_all]\n",
        "    sample_array = df1[\"_sample\"][results_all]\n",
        "\n",
        "    data={\n",
        "        \"_id\": id_array.tolist(), \n",
        "        \"_sample\": sample_array.tolist()\n",
        "    }\n",
        "    print(\"finished - saving as JSON now\")\n",
        "\n",
        "    \n",
        "    savefile = query_dir / (str(filepath.stem) + \".json\")\n",
        "    with open(savefile, 'w') as outfile:\n",
        "        json.dump(data, outfile, indent=2)\n",
        "\n",
        "    print(\"descriptors are replaced by query descriptors in database. save file path: \", savefile)\n",
        "\n",
        "    #record runtime\n",
        "    step_time = time.time() - current_time\n",
        "    print(\"time used:\", step_time)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_LM_xFvI0pfb"
      },
      "source": [
        "#convert list of stored descriptor to music"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D7O3KWqY13sB"
      },
      "source": [
        "wav_dir = output_dir / \"wav_output\"\n",
        "wav_dir.mkdir(parents=True, exist_ok=True)\n",
        "print(\"wav_dir:\", wav_dir)\n",
        "\n",
        "query_file_list = query_dir.rglob(\"*.*\")\n",
        "query_dataframe_list = []\n",
        "for filepath in query_file_list:\n",
        "    to_play = get_dataframe_from_json(filepath)\n",
        "    query_dataframe_list.append((filepath, to_play))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tfF14v-E3QmD"
      },
      "source": [
        "for filepath, to_play in query_dataframe_list:\n",
        "    output_filename = wav_dir / (str(filepath.stem) + \".wav\")\n",
        "    \n",
        "    # output_filename = output_dir / final_wav_filename\n",
        "\n",
        "    if os.path.exists(output_filename):\n",
        "        os.remove(output_filename)\n",
        "\n",
        "    no_samples = len(to_play[\"_sample\"])\n",
        "    out = display(progress(0, no_samples), display_id = True)\n",
        "\n",
        "    concat = AudioSegment.from_wav(to_play[\"_id\"][0])\n",
        "    hop = (hop_length / sr) * 1000\n",
        "    startpos = int((float(to_play[\"_sample\"][0]) / hop_length) * hop)\n",
        "\n",
        "    concat = concat[startpos:startpos + hop]\n",
        "\n",
        "    for x in range(1, no_samples):\n",
        "        print(to_play[\"_id\"][x])\n",
        "        to_concat = AudioSegment.from_wav(to_play[\"_id\"][x])\n",
        "        startpos = int((float(to_play[\"_sample\"][x]) / hop_length) * hop)\n",
        "        if (startpos < crossfade): \n",
        "            thiscrossfade = 0\n",
        "        else: \n",
        "            to_concat = to_concat[startpos - (crossfade / 2):startpos + hop]\n",
        "            thiscrossfade = crossfade\n",
        "        out.update(progress(x + 1, no_samples))\n",
        "\n",
        "        concat = concat.append(to_concat, crossfade = thiscrossfade)\n",
        "\n",
        "    concat.export(output_filename, format = \"wav\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}